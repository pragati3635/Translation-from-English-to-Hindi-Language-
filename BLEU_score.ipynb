{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport unicodedata\nimport torch\nfrom string import digits\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T12:19:56.351558Z","iopub.execute_input":"2022-05-02T12:19:56.35229Z","iopub.status.idle":"2022-05-02T12:19:56.357666Z","shell.execute_reply.started":"2022-05-02T12:19:56.352252Z","shell.execute_reply":"2022-05-02T12:19:56.356617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAD_TOKEN = 0\nSOS_TOKEN = 1\nEOS_TOKEN = 2","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:19:59.535677Z","iopub.execute_input":"2022-05-02T12:19:59.535926Z","iopub.status.idle":"2022-05-02T12:19:59.540115Z","shell.execute_reply.started":"2022-05-02T12:19:59.535899Z","shell.execute_reply":"2022-05-02T12:19:59.539401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from pytorch's documentation website on NLP\nclass Dictionary:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n        self.n_count = 3\n\n    def add_sentence(self, sentence):\n        for word in sentence.split(' '):\n            self.add_word(word)\n\n    def add_word(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_count\n            self.word2count[word] = 1\n            self.index2word[self.n_count] = word \n            self.n_count += 1\n        else:\n            self.word2count[word] += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:03.255667Z","iopub.execute_input":"2022-05-02T12:20:03.25633Z","iopub.status.idle":"2022-05-02T12:20:03.263143Z","shell.execute_reply.started":"2022-05-02T12:20:03.25629Z","shell.execute_reply":"2022-05-02T12:20:03.262381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing of english string\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\ndef normalizeString(s):\n    s = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', s)  #removal url\n    s = unicodeToAscii(s.lower().strip()) # small letters \n    s = re.sub(r\"([.!?])\", r\" \\1\", s) # removal puntuations\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #removal digits\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:09.211638Z","iopub.execute_input":"2022-05-02T12:20:09.212311Z","iopub.status.idle":"2022-05-02T12:20:09.217809Z","shell.execute_reply.started":"2022-05-02T12:20:09.212269Z","shell.execute_reply":"2022-05-02T12:20:09.217136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Preprocessing of hindi sentences\nremove_digits = str.maketrans('', '', digits)\nabc = string.punctuation\nabc = abc[:12]+abc[14:]\n\ndef normalizeString_hindi(s):\n    s = re.compile(r'https?://\\S+|www\\.\\S+').sub(r'', s)\n    s = re.sub(\"\\n\", \"\", s)\n    s = re.sub(\"-\",\" \",s)\n    s = re.sub(\"“\",\" \",s)\n    s = re.sub(\"”\",\" \",s)\n    s = re.sub(\"’’\",\" \",s)\n    s = re.sub(\"‘‘\",\" \",s)\n    s = re.sub(\"‘\",\" \",s)\n    s = re.sub(\"’\",\" \",s)\n    s = s.translate(str.maketrans('', '', abc ))\n    s = re.sub(\"'\", ' ', s)\n    s = s.translate(remove_digits)\n    s = re.sub(\"[२३०८१५७९४६]\", \" \", s)\n    s = re.sub(\"[a-zA-Z]\", \" \", s)\n    s= s.strip()\n    s = re.sub(\" +\", \" \", s)\n    return s","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:15.540379Z","iopub.execute_input":"2022-05-02T12:20:15.540928Z","iopub.status.idle":"2022-05-02T12:20:15.55035Z","shell.execute_reply.started":"2022-05-02T12:20:15.540889Z","shell.execute_reply":"2022-05-02T12:20:15.549318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_files(lang1, lang2, reverse, MAX_FILE_SIZE, MAX_LENGTH):\n    #load first language to list\n    lang1_list = []\n    lang1_file = open('../input/' + lang1 + '1' + lang2 + '/' + lang1 + '.txt', 'r', encoding='utf8')\n    for i, (line) in enumerate(lang1_file):\n        if i < MAX_FILE_SIZE:\n            lang1_list.append(line)\n        else:\n            break\n    # load second langauge to list\n    lang2_list = []\n    lang2_file = open('../input/' + lang1 + '1' + lang2 + '/' + lang2 + '.txt', 'r', encoding='utf8')\n    for i, (line) in enumerate(lang2_file):\n        if i < MAX_FILE_SIZE:\n            lang2_list.append(line)\n        else:\n            break\n    #preprocess strings\n\n    if(lang1 == \"hindi\"):\n        lang1_normalized = list(map(normalizeString_hindi, lang1_list))\n    else:\n        lang1_normalized = list(map(normalizeString, lang1_list))\n\n    if(lang2 == \"hindi\"):\n        lang2_normalized = list(map(normalizeString_hindi, lang2_list))\n    else:\n        lang2_normalized = list(map(normalizeString, lang2_list))\n\n    lang1_sentences = list()\n    lang2_sentences = list()\n\n    for i in range(len(lang1_normalized)):\n        tokens1 = lang1_normalized[i].split(' ')\n        tokens2 = lang2_normalized[i].split(' ')\n        if len(tokens1) <= MAX_LENGTH and len(tokens2) <= MAX_LENGTH:\n            lang1_sentences.append(lang1_normalized[i])\n            lang2_sentences.append(lang2_normalized[i])\n\n    del lang1_normalized\n    del lang2_normalized\n\n    if reverse:\n        input_dic = Dictionary(lang2)\n        output_dic = Dictionary(lang1)\n        return input_dic, output_dic, lang2_sentences, lang1_sentences","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:20.046122Z","iopub.execute_input":"2022-05-02T12:20:20.048376Z","iopub.status.idle":"2022-05-02T12:20:20.065944Z","shell.execute_reply.started":"2022-05-02T12:20:20.048332Z","shell.execute_reply":"2022-05-02T12:20:20.065196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize\ndef tokenize(sentence, dictionary, MAX_LENGTH=60):\n    split_sentence = [word for word in sentence.split(' ')]\n    token = [SOS_TOKEN]\n    for word in sentence.split(' '):\n        if word in dictionary.word2index:\n            token.append(dictionary.word2index[word])\n        else:\n            token.append(PAD_TOKEN)\n    #token += [dictionary.word2index[word] for word in sentence.split(' ')]\n    token.append(EOS_TOKEN)\n    token += [PAD_TOKEN]*(MAX_LENGTH - len(split_sentence))\n    return token","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:23.990712Z","iopub.execute_input":"2022-05-02T12:20:23.991261Z","iopub.status.idle":"2022-05-02T12:20:23.997286Z","shell.execute_reply.started":"2022-05-02T12:20:23.991216Z","shell.execute_reply":"2022-05-02T12:20:23.99662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef load_batches(input_lang, output_lang, batch_size, device):\n    data_loader = []\n    for i in range(0, len(input_lang), batch_size):\n        seq_length = min(len(input_lang) - batch_size, batch_size)\n        input_batch = input_lang[i:i+seq_length][:]\n        target_batch = output_lang[i:i+seq_length][:]\n        input_tensor = torch.LongTensor(input_batch).to(device)\n        target_tensor = torch.LongTensor(target_batch).to(device)\n        data_loader.append([input_tensor, target_tensor])\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:27.690285Z","iopub.execute_input":"2022-05-02T12:20:27.690526Z","iopub.status.idle":"2022-05-02T12:20:27.696986Z","shell.execute_reply.started":"2022-05-02T12:20:27.690499Z","shell.execute_reply":"2022-05-02T12:20:27.695565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\n\nclass MultiHeadAttentionLayer(nn.Module):\n    def __init__(self, hidden_size, n_heads, dropout, device):\n        super().__init__()\n        \n        assert hidden_size % n_heads == 0\n        \n        self.hidden_size = hidden_size\n        self.n_heads = n_heads\n        self.head_size = hidden_size // n_heads\n        \n        self.fc_query = nn.Linear(hidden_size, hidden_size)\n        self.fc_key = nn.Linear(hidden_size, hidden_size)\n        self.fc_value = nn.Linear(hidden_size, hidden_size)\n        self.fc_out = nn.Linear(hidden_size, hidden_size)\n    \n        self.dp = nn.Dropout(dropout)\n        \n        self.coefficient = torch.sqrt(torch.FloatTensor([self.head_size])).to(device)\n        \n    def forward(self, query, key, value, mask=None):\n        b_size = query.shape[0]\n   \n        query_output = self.fc_query(query)\n        key_output = self.fc_key(key)\n        value_output = self.fc_value(value)\n     \n        query_output = query_output.view(b_size, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n        key_output = key_output.view(b_size, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n        value_output = value_output.view(b_size, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n      \n        energy = torch.matmul(query_output, key_output.permute(0, 1, 3, 2)) / self.coefficient\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, -1e10)\n        \n        attention = torch.softmax(energy, dim = -1)    \n        output = torch.matmul(self.dp(attention), value_output)\n        output = output.permute(0, 2, 1, 3).contiguous()\n        output = output.view(b_size, -1, self.hidden_size)  \n        output = self.fc_out(output)\n        return output, attention\n\n\n\nclass FeedForwardLayer(nn.Module):\n    def __init__(self, hidden_size, ff_size, dropout):\n        super().__init__()\n\n        self.ff_layer = nn.Sequential(\n            nn.Linear(hidden_size, ff_size),\n            nn.ReLU(),\n            \n            nn.Dropout(dropout),\n            nn.Linear(ff_size, hidden_size)\n        )\n        \n    def forward(self, input):\n        output = self.ff_layer(input)\n        return output\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, hidden_size, n_heads, ff_size,  dropout, device):\n        super().__init__()\n        \n        self.self_atten = MultiHeadAttentionLayer(hidden_size, n_heads, dropout, device)\n        self.self_atten_norm = nn.LayerNorm(hidden_size)\n        self.ff_layer = FeedForwardLayer(hidden_size, ff_size, dropout)\n        self.dp = nn.Dropout(dropout)\n        self.ff_layer_norm = nn.LayerNorm(hidden_size)\n        \n    def forward(self, input, input_mask):\n        #self attention\n        atten_result, _ = self.self_atten(input, input, input, input_mask)\n        \n        atten_norm = self.self_atten_norm(input + self.dp(atten_result))\n        ff_result = self.ff_layer(atten_norm)\n        \n        output = self.ff_layer_norm(atten_norm + self.dp(ff_result))\n        return output\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers, n_heads, ff_size,dropout, device, MAX_LENGTH=100):\n        super().__init__()\n\n        self.device = device\n        \n        self.te = nn.Embedding(input_size, hidden_size)\n        self.pe = nn.Embedding(MAX_LENGTH, hidden_size)\n        \n        encoding_layers = []\n        for _ in range(n_layers):\n            encoding_layers.append(EncoderLayer(hidden_size, n_heads, ff_size, dropout, device))\n        self.encode_sequence = nn.Sequential(*encoding_layers)\n        \n        self.dp = nn.Dropout(dropout)\n        \n        self.coefficient = torch.sqrt(torch.FloatTensor([hidden_size])).to(device)\n        \n    def forward(self, input, input_mask):\n        b_size = input.shape[0]\n        input_size = input.shape[1]\n        \n        pos = torch.arange(0, input_size).unsqueeze(0).repeat(b_size, 1).to(self.device)\n        input = self.dp((self.te(input) * self.coefficient) + self.pe(pos))\n\n        for layer in self.encode_sequence:\n            input = layer(input, input_mask)\n  \n        return input\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, hidden_size, n_heads, ff_size, dropout, device):\n        super().__init__()\n        \n        self.self_atten = MultiHeadAttentionLayer(hidden_size, n_heads, dropout, device)\n        self.self_atten_norm = nn.LayerNorm(hidden_size)\n        self.encoder_atten = MultiHeadAttentionLayer(hidden_size, n_heads, dropout, device)\n        self.encoder_atten_norm = nn.LayerNorm(hidden_size)\n        self.ff_layer = FeedForwardLayer(hidden_size, ff_size, dropout)\n        self.ff_layer_norm = nn.LayerNorm(hidden_size)\n        self.dp = nn.Dropout(dropout)\n        \n    def forward(self, target, encoded_input, target_mask, input_mask):\n        #self attention\n        atten_result, _ = self.self_atten(target, target, target, target_mask)\n        \n        atten_norm = self.self_atten_norm(target + self.dp(atten_result))\n\n        atten_encoded, attention = self.encoder_atten(atten_norm, encoded_input, encoded_input, input_mask)\n        \n        encoded_norm = self.encoder_atten_norm(atten_norm + self.dp(atten_encoded))\n\n        ff_result = self.ff_layer(encoded_norm)\n\n        output = self.ff_layer_norm(encoded_norm + self.dp(ff_result))\n\n        return output, attention\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, hidden_size, n_layers, n_heads, ff_size, dropout, device, MAX_LENGTH=100):\n        super().__init__()\n        \n        self.device = device\n        \n        self.te = nn.Embedding(output_size, hidden_size)\n        self.pe = nn.Embedding(MAX_LENGTH, hidden_size)\n\n        decoding_layers = []\n        for _ in range(n_layers):\n            decoding_layers.append(DecoderLayer(hidden_size, n_heads, ff_size, dropout, device))\n        \n        self.decode_sequence = nn.Sequential(*decoding_layers) \n        \n        self.fc_out = nn.Linear(hidden_size, output_size)\n        \n        self.dp = nn.Dropout(dropout)\n        \n        self.coefficient = torch.sqrt(torch.FloatTensor([hidden_size])).to(device)\n        \n    def forward(self, target, encoded_input, target_mask, input_mask):    \n        b_size = target.shape[0]\n        target_size = target.shape[1]\n        \n        pos = torch.arange(0, target_size).unsqueeze(0).repeat(b_size, 1).to(self.device)\n        target = self.dp((self.te(target) * self.coefficient) + self.pe(pos))\n        for layer in self.decode_sequence:\n            target, attention = layer(target, encoded_input, target_mask, input_mask)\n\n        output = self.fc_out(target)\n        return output, attention\n\nclass Transformer(nn.Module):\n    def __init__(self, encoder, decoder, device, padding_index=0):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.padding_index = padding_index\n        self.device = device\n        \n    def make_input_mask(self, input):\n\n        input_mask = (input != self.padding_index).unsqueeze(1).unsqueeze(2)\n        return input_mask\n    \n    def make_target_mask(self, target):\n\n        target_pad_mask = (target != self.padding_index).unsqueeze(1).unsqueeze(2)\n        target_sub_mask = torch.tril(torch.ones((target.shape[1], target.shape[1]), device = self.device)).bool()\n        target_mask = target_pad_mask & target_sub_mask\n        return target_mask\n\n    def forward(self, input, target):   \n        input_mask = self.make_input_mask(input)\n        target_mask = self.make_target_mask(target)\n\n        #encoder feed through\n        encoded_input = self.encoder(input, input_mask)\n\n        #decoder feed_through\n        output, attention = self.decoder(target, encoded_input, target_mask, input_mask)\n\n        return output, attention","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:41.908408Z","iopub.execute_input":"2022-05-02T12:20:41.908731Z","iopub.status.idle":"2022-05-02T12:20:41.950449Z","shell.execute_reply.started":"2022-05-02T12:20:41.908696Z","shell.execute_reply":"2022-05-02T12:20:41.949726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DO NOT RUN THIS CELL !!!!!\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np \nimport time\nimport matplotlib.pyplot as plt\nimport argparse\nimport torch.optim as optim\nfrom random import shuffle\nimport pickle\n\ntorch.cuda.empty_cache()\n\nlang1 = \"hindi\"\nlang2 = \"english\"\nreverse = 1\nMAX_LENGTH = 60\nMAX_FILE_SIZE = 200000\nbatch_size = 64\nlr = 0.0005\nhidden_size = 256\nencoder_layers = 3\ndecoder_layers = 3\nencoder_heads = 8\ndecoder_heads = 8\nencoder_ff_size = 512\ndecoder_ff_size = 512\nencoder_dropout = 0.1\ndecoder_dropout = 0.1\ndevice = 'cuda'\nepochs = 10\nsaved_model_directory = '/kaggle/working/'\n\ninput_lang_dic, output_lang_dic, input_lang_list, output_lang_list = load_files(lang1, lang2,reverse, MAX_FILE_SIZE, MAX_LENGTH)\n\nfor sentence in input_lang_list:\n    input_lang_dic.add_sentence(sentence)\n\nfor sentence in output_lang_list:\n    output_lang_dic.add_sentence(sentence)\n    \ndef save_dictionary(self, dictionary, input):\n    if input:\n        with open('/kaggle/working/' + 'input_dic.pkl', 'wb') as f:\n            pickle.dump(dictionary, f, pickle.HIGHEST_PROTOCOL)\n    else:\n        with open('/kaggle/working/' + 'output_dic.pkl', 'wb') as f:\n            pickle.dump(dictionary, f, pickle.HIGHEST_PROTOCOL)\n            \ndef initialize_weights(self, model):\n    if hasattr(model, 'weight') and model.weight.dim() > 1:\n        nn.init.xavier_uniform_(model.weight.data)\n    \nsave_dictionary(input_lang_dic, True)\nsave_dictionary(output_lang_dic,False)\n\ntokenized_input_lang = [tokenize(sentence, input_lang_dic, MAX_LENGTH) for sentence in input_lang_list]\ntokenized_output_lang = [tokenize(sentence, output_lang_dic, MAX_LENGTH) for sentence in output_lang_list]\n\ndata_loader = load_batches(tokenized_input_lang, tokenized_output_lang, batch_size, device)\n\ninput_size = input_lang_dic.n_count\noutput_size = output_lang_dic.n_count\n\nencoder_part = Encoder(input_size, hidden_size, encoder_layers, encoder_heads, encoder_ff_size, encoder_dropout, device)\ndecoder_part = Decoder(output_size, hidden_size, decoder_layers, decoder_heads, decoder_ff_size, decoder_dropout, device)\n\ntransformer = Transformer(encoder_part, decoder_part, device, PAD_TOKEN).to(device)\ntransformer.apply(initialize_weights)\n\nloss_func = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\noptimizer = optim.Adam(transformer.parameters(), lr=lr)\n            \n    def train(epochs, saved_model_directory):\n        start_time = time.time()\n\n        for epoch in range(epochs):\n            shuffle(data_loader)\n\n            start_time = time.time()\n            train_loss = 0\n\n            for input, target in data_loader:\n                \n                optimizer.zero_grad()\n\n                output, _ = self.transformer(input, target[:,:-1])\n                output_dim = output.shape[-1]\n\n                output = output.contiguous().view(-1, output_dim)\n                target = target[:,1:].contiguous().view(-1)\n\n                loss = self.loss_func(output, target)\n\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.transformer.parameters(), 1)\n                self.optimizer.step()\n\n                train_loss += loss.item()\n                \n            train_loss /= len(self.data_loader)\n\n            end_time = int(time.time() - start_time)\n            torch.save(self.transformer.state_dict(), '/kaggle/working/transformer_model_{}.pt'.format(epoch))\n\n            print('Epoch: {},   Time: {}s,  Estimated {} seconds remaining.'.format(epoch, end_time, (epochs-epoch)*end_time))\n            print('\\tTraining Loss: {:.4f}\\n'.format(train_loss))\n        print('Training finished!')\n\ntrain(epochs, saved_model_directory)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport argparse\nimport pickle\nimport sys\n\n\nSOS_TOKEN = 1\n\ndef load_dictionary(directory):\n    with open(directory, 'rb') as f:\n        return pickle.load(f)\n\ndef translate_sentence(sentence, input_dic, output_dic, model, device, max_len):\n    \n    model.eval()\n    normalized_sentence = normalizeString(sentence)\n    tokens = tokenize(normalized_sentence, input_dic)\n    input_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n    input_mask = model.make_input_mask(input_tensor)\n    \n    with torch.no_grad():\n        encoded_input = model.encoder(input_tensor, input_mask)\n\n    target_tokens = [SOS_TOKEN]\n\n    for i in range(max_len):\n\n        target_tensor = torch.LongTensor(target_tokens).unsqueeze(0).to(device)\n        target_mask = model.make_target_mask(target_tensor)\n    \n        with torch.no_grad():\n            output, attention = model.decoder(target_tensor, encoded_input, target_mask, input_mask)\n        \n        pred_token = output.argmax(2)[:,-1].item()\n        target_tokens.append(pred_token)\n        if pred_token == EOS_TOKEN:\n            break\n    \n    target_results = [output_dic.index2word[i] for i in target_tokens]\n    #print(target_results)\n    return ' '.join(target_results[1:-1]), attention\n\ndef translate(text):\n\n    input_text = text\n    input_lang = \"english\"\n    output_lang = \"hindi\"\n    models_dir = \"../input/my-model\"\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    #hyper parameters\n    MAX_LENGTH = 60\n    hidden_size = 256\n    encoder_layers = 3\n    decoder_layers = 3\n    encoder_heads = 8\n    decoder_heads = 8\n    encoder_ff_size = 512\n    decoder_ff_size = 512\n    encoder_dropout = 0.1\n    decoder_dropout = 0.1\n\n    transformer_location = models_dir + '/' \n\n    #load dictionaries\n    input_lang_dic = load_dictionary(transformer_location + 'input_dic.pkl')\n    output_lang_dic = load_dictionary(transformer_location + 'output_dic.pkl')\n\n    input_size = input_lang_dic.n_count\n    output_size = output_lang_dic.n_count\n    \n    #define models\n    encoder_part = Encoder(input_size, hidden_size, encoder_layers, encoder_heads, encoder_ff_size, encoder_dropout, device)\n    decoder_part = Decoder(output_size, hidden_size, decoder_layers, decoder_heads, decoder_ff_size, decoder_dropout, device)\n\n    translator = Transformer(encoder_part, decoder_part, device).to(device)\n    translator.load_state_dict(torch.load(transformer_location + 'transformer_model_9.pt'))\n\n    translation, attention = translate_sentence(input_text, input_lang_dic, output_lang_dic, translator, device, MAX_LENGTH)\n    #print(input_lang + ': ' + input_text)\n    #print('\\n' + output_lang + ': ')\n    return translation\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:20:49.422648Z","iopub.execute_input":"2022-05-02T12:20:49.42306Z","iopub.status.idle":"2022-05-02T12:20:49.437551Z","shell.execute_reply.started":"2022-05-02T12:20:49.423024Z","shell.execute_reply":"2022-05-02T12:20:49.436463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = translate(\"The Ukrainian president's top aide says he does not believe Moscow will launch a nuclear strike against his country because Russia's decision-makers know they could end up dying in a global nuclear war.Andriy Yermak also told Sky News he hoped Ukraine would soon defeat Russia’s invading forces and secure victory.\")\ns","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:18:01.894402Z","iopub.execute_input":"2022-05-02T13:18:01.895125Z","iopub.status.idle":"2022-05-02T13:18:03.605692Z","shell.execute_reply.started":"2022-05-02T13:18:01.895085Z","shell.execute_reply":"2022-05-02T13:18:03.605032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s= translate(\"How are you ? \")\ns","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:06:30.317065Z","iopub.execute_input":"2022-05-02T13:06:30.317656Z","iopub.status.idle":"2022-05-02T13:06:31.64627Z","shell.execute_reply.started":"2022-05-02T13:06:30.317604Z","shell.execute_reply":"2022-05-02T13:06:31.645455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang2_list = []\nlang2_file = open('../input/test-set/test_hindi.txt', 'r')\nfor i, (line) in enumerate(lang2_file):\n    if i<34 :\n        lang2_list.append(line)\n    else:\n        break\n\nlang2_normalized = list(map(normalizeString_hindi, lang2_list))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:44:12.465241Z","iopub.execute_input":"2022-05-02T08:44:12.465504Z","iopub.status.idle":"2022-05-02T08:44:12.476697Z","shell.execute_reply.started":"2022-05-02T08:44:12.465477Z","shell.execute_reply":"2022-05-02T08:44:12.475905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reference = list()\nfor i in lang2_normalized:\n    new_list = list()\n    for word in i.split(' '):\n        new_list.append(word)\n    reference.append(new_list)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:44:16.534783Z","iopub.execute_input":"2022-05-02T08:44:16.535411Z","iopub.status.idle":"2022-05-02T08:44:16.540615Z","shell.execute_reply.started":"2022-05-02T08:44:16.53537Z","shell.execute_reply":"2022-05-02T08:44:16.539693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translated_sentence = list()\nlang1_file = open('../input/test-set/test_english.txt', 'r')\nfor i, (line) in enumerate(lang1_file):\n    if i < 34 :\n        translated_sentence.append(translate(line))\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:44:20.639487Z","iopub.execute_input":"2022-05-02T08:44:20.640201Z","iopub.status.idle":"2022-05-02T08:45:03.779101Z","shell.execute_reply.started":"2022-05-02T08:44:20.640165Z","shell.execute_reply":"2022-05-02T08:45:03.77833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tl = list()\nfor i in translated_sentence:\n    new_list = list()\n    for word in i.split(' '):\n        new_list.append(word)\n    tl.append(new_list)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:45:33.208556Z","iopub.execute_input":"2022-05-02T08:45:33.208913Z","iopub.status.idle":"2022-05-02T08:45:33.217441Z","shell.execute_reply.started":"2022-05-02T08:45:33.208838Z","shell.execute_reply":"2022-05-02T08:45:33.216672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nprint(reference[9])\nprint(tl[9])\nBLEUscore = nltk.translate.bleu_score.sentence_bleu([reference[9]], tl[9])\nprint(BLEUscore)\nprint(len(tl))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:47:07.647218Z","iopub.execute_input":"2022-05-02T08:47:07.647473Z","iopub.status.idle":"2022-05-02T08:47:07.653315Z","shell.execute_reply.started":"2022-05-02T08:47:07.647446Z","shell.execute_reply":"2022-05-02T08:47:07.652639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(reference)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:03:48.550207Z","iopub.execute_input":"2022-05-02T08:03:48.550955Z","iopub.status.idle":"2022-05-02T08:03:48.557485Z","shell.execute_reply.started":"2022-05-02T08:03:48.55092Z","shell.execute_reply":"2022-05-02T08:03:48.55545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\ntotal = list()\ncount =0\nfor i in range(0,34):\n    mini = min(len(reference[i]),len(tl[i]))\n    if mini>4 or mini==4:\n               BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference[i]], tl[i])\n               count+=1\n               total.append(BLEUscore)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:03:53.924712Z","iopub.execute_input":"2022-05-02T08:03:53.924983Z","iopub.status.idle":"2022-05-02T08:03:53.937097Z","shell.execute_reply.started":"2022-05-02T08:03:53.924952Z","shell.execute_reply":"2022-05-02T08:03:53.936399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(total)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:03:59.509732Z","iopub.execute_input":"2022-05-02T08:03:59.510302Z","iopub.status.idle":"2022-05-02T08:03:59.517494Z","shell.execute_reply.started":"2022-05-02T08:03:59.510263Z","shell.execute_reply":"2022-05-02T08:03:59.516774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:04:02.679766Z","iopub.execute_input":"2022-05-02T08:04:02.680064Z","iopub.status.idle":"2022-05-02T08:04:02.685542Z","shell.execute_reply.started":"2022-05-02T08:04:02.680032Z","shell.execute_reply":"2022-05-02T08:04:02.6848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(total)/count","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:04:05.519528Z","iopub.execute_input":"2022-05-02T08:04:05.519798Z","iopub.status.idle":"2022-05-02T08:04:05.525339Z","shell.execute_reply.started":"2022-05-02T08:04:05.519768Z","shell.execute_reply":"2022-05-02T08:04:05.524478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = 'मौके पर पहुंची पुलिस ने तफ्तीश की.'\nfor word in sentence.split(' '):\n  print(word)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:56:32.845557Z","iopub.execute_input":"2022-05-01T14:56:32.846116Z","iopub.status.idle":"2022-05-01T14:56:32.852662Z","shell.execute_reply.started":"2022-05-01T14:56:32.846071Z","shell.execute_reply":"2022-05-01T14:56:32.851647Z"},"trusted":true},"execution_count":null,"outputs":[]}]}